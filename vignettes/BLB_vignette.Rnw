\documentclass{article}

\usepackage[style=authoryear, backend=bibtex]{biblatex}
\bibliography{bootstrap.bib}

\usepackage{latexsym,amsmath,amssymb}
%\VignetteEngine{knitr::knitr}

\title{Bootstrap Methods in R and C}
\author{Ella Kaye \and Xenia Miscouridou}

\begin{document}
\SweaveOpts{concordance=TRUE}

\maketitle

\begin{abstract}
We describe a package \texttt{BLB} in R which implements the Bag of Little Bootstraps \parencite{Kleiner2014}.

The package is available from \texttt{https://github.com/EllaKaye/BLB}.
\end{abstract}

\section{Introduction}
The bootstrap \parencite{Efron1979}, is a statistical tool which gives measures of accuracy of estimators, and can therefore be used to draw inference about the parameters of the sampling distribution. It is a powerful and widely applicable tool. The bootstrap is at the interface between statistical inference and computation, and it was developed at a time when advances in computing technology allowed this computationally intensive method to be used in practice. Shortly after, the Bayesian analogue of the bootstrap was introduced \parencite{Rubin1981}. More recently, in this era of `big data', statistical methodology is again being developed alongside developments in computing technology. Parallel and distributed computing architectures now make it possible to extend the bootstrap so that it can be applied to massive datasets, the so-called bag of little bootstraps \parencite{Kleiner2014}.

We have developed a package, \texttt{Bootstrap}, with functions to implement the bootstrap, Bayesian bootstrap and bag of little bootstraps. The package can be obtained from GitHub.



\section{The Bootstrap}
Suppose we observe a sample of $n$ iid realisations  $x_1,\ldots, x_n \sim P$, for some probability measure $P$. Let $\theta$ be a parameter of the distribution, and $\hat\theta_n$ be an estimator of $\theta$. The goal of the bootstrap is to obtain an assessment, $\xi$, of the quality of the estimator. For example, $\theta$ could be the median, and $\xi$ the standard error. To obtain a bootstrap estimate, we proceed as follows:

\begin{enumerate}
\item Repeatedly ($B$ times) sample $n$ points with replacement from the original
dataset, giving bootstrap replications (resamples) $(x_1^{*(i)},\ldots,x_n^{*(i)}),\ i=1,\ldots,B$
\item Compute $\hat\theta_n^{*(i)}$ on each of the $B$ resamples.
\item Compute $\xi^*=\xi(\hat\theta_n^{*(1)},\ldots,\hat\theta_n^{*(B)})$ as our bootstrap estimate of $\xi$.
\end{enumerate}



\section{Bag of Little Bootstraps}
The original bootstrap arose around the time when increases in computing power allowed the development of statistical tools that had previously been too computationally expensive. In recent years, there has been an influx of `big data', alongside the development of parallel computing architectures. \textcite{Kleiner2014} have developed a scalable bootstrap for massive data, known as the Bag of Little Bootstraps (BLB). With massive datasets, the bootstrap's need for recomputation on resamples of the same size as the original dataset is problematic. Rather than obtain bootstrap samples from the whole dataset, the BLB breaks down the process as follows:

\begin{enumerate}
\item Repeatedly ($s$ times) subsample $b(n) < n$ data points \emph{without replacement} from the original dataset of size $n$.
\item For each of the $s$ subsamples, do the following:
\begin{enumerate}
\item Repeatedly ($r$ times) resample $n$ data points \emph{with replacement} from the subsample.
\item Compute $\hat\theta_n^*$ on each resample.
\item Compute an estimate of $\xi$ based on these $r$ realisations of $\hat\theta_n^*$.
\end{enumerate}
\item We now have one estimate of $\xi$ per subsample, $\xi^*_1, \ldots, \xi^*_s$. Output their average, $\xi^*$, as the final estimate of $\xi$ for $\hat\theta_n$.
\end{enumerate}

%by taking $s$ subsamples of the original dataset, each of size $b$, where typically $b \ll n$. Each of these subsamples is generated from the original dataset WITHOUT replacement.

\textcite{Kleiner2014} recommend taking $b(n) = n^{\gamma}$, where $\gamma \in [0.5,1]$. This procedure dramatically reduces the size of each resample. For example, if $n$ = 1 million and $\gamma=0.6$, the size of the original dataset may be around 1TB, with a bootstrap resample typically occupying approximately 632GB, and a BLB subsample or resample occupying just 4GB.

The Bootstrap package contains three functions which implement BLB in different cases, \texttt{BLB.1d, BLB.multi} and \texttt{BLB.adapt}. The function \texttt{BLB.1d} implements the simplest version of BLB. It takes as input: a 1-dimensional dataset in a vector; $\gamma$, which controls the value of $b$; a function FUN, which computes the parameter estimate, $\hat\theta_n$. It also takes as arguments $s$ and $r$, which default to 20 and 100 respectively (\textcite{Kleiner2014} demonstates that these values are likely as large as they'll need to be to obtain convergence). The function returns a BLB estimate of $\xi$, which is set as the standard error. When $n=500,000$ the function takes under two minutes to execute (although we compute with a smaller sample here).

\printbibliography
\end{document}
